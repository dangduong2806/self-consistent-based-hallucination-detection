import json
import torch
from torch.utils.data import Dataset, random_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
import yaml
import gc

# C·∫•u h√¨nh
config_path = "configs/main_config.yaml"

class PRMDataset(Dataset):
    def __init__(self, jsonl_path, tokenizer, max_len=512):    
        self.data = []
        self.tokenized_data = []
        self.tokenizer = tokenizer
        self.max_len = max_len

        # Mapping t·ª´ Rating c·ªßa dataset sang Label c·ªßa Model (0, 1, 2)
        # Rating -1 -> Label 0 (Sai)
        # Rating  0 -> Label 1 (Trung t√≠nh)
        # Rating  1 -> Label 2 (ƒê√∫ng)
        self.label_map = {-1: 0, 0: 1, 1: 2}

        # Load data PRM800K (Phase 2 - label t·ª´ng b∆∞·ªõc)
        with open(jsonl_path, 'r') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    
                    # Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o c∆° b·∫£n
                    if 'question' not in item or 'label' not in item or 'steps' not in item['label']:
                        continue
                    
                    problem = item['question']['problem']
                    steps = item['label']['steps']

                    # History kh·ªüi ƒë·∫ßu b·∫±ng ƒë·ªÅ b√†i
                    history_context = problem

                    for step in steps:
                        # 1. T·∫°o D·ªØ li·ªáu Training (H·ªçc c·∫£ ƒê√∫ng v√† Sai)
                        if step.get('completions') is None: continue

                        for comp in step['completions']:
                            if comp['rating'] is None: continue

                            rating = comp.get('rating')
                            label = self.label_map[rating]
                            # Input cho DeBERTa: Context + [SEP] + Candidate
                            text = f"{history_context} [SEP] {comp['text']}"

                            self.data.append({
                                    'text': text,
                                    'label': label
                                })
                        # 2. C·∫≠p nh·∫≠t History cho b∆∞·ªõc sau (Ch·ªâ l·∫•y b∆∞·ªõc ƒë∆∞·ª£c ch·ªçn l√†m m·∫°ch ch√≠nh)
                        # Sample c·ªßa b·∫°n c√≥ tr∆∞·ªùng 'chosen_completion' (VD: 0, 1, 2...)
                        # ƒê√¢y l√† ch·ªâ m·ª•c c·ªßa b∆∞·ªõc ƒëi ti·∫øp theo m·∫°ch truy·ªán
                        chosen_idx = step.get('chosen_completion')
                        if chosen_idx is not None and chosen_idx < len(step['completions']):
                            # L·∫•y text c·ªßa b∆∞·ªõc ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ c·ªông v√†o l·ªãch s·ª≠
                            chosen_step_text = step['completions'][chosen_idx]['text']
                            history_context += " " + chosen_step_text
                        else:
                            # Fallback: N·∫øu kh√¥ng c√≥ chosen_completion, m·ªõi d√πng logic t√¨m c√°i ƒë√∫ng ƒë·∫ßu ti√™n
                            for comp in step['completions']:
                                if comp['rating'] == 1:
                                    history_context += " " + comp['text']
                                    break
                except Exception as e:
                    print(f"Error details: {str(e)}")
                    import traceback
                    traceback.print_exc()

        print(f"‚úÖ ƒê√£ ƒë·ªçc xong {len(self.data)} m·∫´u raw. B·∫Øt ƒë·∫ßu Tokenize h√†ng lo·∫°t...")

        # B∆Ø·ªöC 2: Tokenize H√†ng lo·∫°t (Batch Tokenization) - T·ªëc ƒë·ªô X100 l·∫ßn
        # Thay v√¨ tokenize t·ª´ng d√≤ng, ta gom l·∫°i x·ª≠ l√Ω lu√¥n
        batch_size = 2000
        for i in range(0, len(self.data), batch_size):
            batch = self.data[i : i + batch_size]
            texts = [x['text'] for x in batch]
            labels = [x['label'] for x in batch]
            
            # Tokenize batch n√†y
            encodings = tokenizer(
                texts,
                truncation=True,
                max_length=max_len,
                add_special_tokens=True,
                return_attention_mask=True,
                # KH√îNG padding ·ªü ƒë√¢y ƒë·ªÉ ti·∫øt ki·ªám RAM, ƒë·ªÉ DataCollator lo
            )
            
            # L∆∞u v√†o self.data
            for j in range(len(texts)):
                self.tokenized_data.append({
                    'input_ids': encodings['input_ids'][j],
                    'attention_mask': encodings['attention_mask'][j],
                    'label': labels[j]
                })

        # B∆Ø·ªöC 3: D·ªåN D·∫∏P RAM (QUAN TR·ªåNG NH·∫§T)
        del self.data
        gc.collect() # √âp Python gi·∫£i ph√≥ng RAM ngay l·∫≠p t·ª©c
        n = len(self.tokenized_data)
        self.tokenized_data = self.tokenized_data[: 1 * n // 6]
        print(f"üéâ S·∫µn s√†ng train! T·ªïng s·ªë m·∫´u: {len(self.tokenized_data)}")

    def __len__(self):
        return len(self.tokenized_data)

    # def __getitem__(self, idx):
    #     item = self.data[idx]
    #     encoding = self.tokenizer(
    #         item['text'],
    #         truncation=True,
    #         max_length=self.max_len,
    #         # padding="max_length",
    #         # return_tensors="pt"
    #     )
    #     return {
    #         "input_ids": encoding["input_ids"],
    #         "attention_mask": encoding["attention_mask"],
    #         # "labels": torch.tensor(item['label'], dtype=torch.long)
    #         "labels": item['label']
    #     }
    def __getitem__(self, idx):
        # H√†m n√†y gi·ªù ch·ªâ vi·ªác l·∫•y d·ªØ li·ªáu c√≥ s·∫µn, kh√¥ng c·∫ßn t√≠nh to√°n g√¨ c·∫£ -> Si√™u nhanh
        return {
            "input_ids": self.tokenized_data[idx]['input_ids'],
            "attention_mask": self.tokenized_data[idx]['attention_mask'],
            "labels": self.tokenized_data[idx]['label']
        }

def train():
    # 1. Load Config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    MODEL_NAME = config['tool']['name1']
    OUTPUT_DIR = config['tool']['output_dir1']
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True) # tokenize nhanh h∆°n  
    
    # --- S·ª¨A QUAN TR·ªåNG: num_labels=3 ---
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, 
        num_labels=3,  # Model s·∫Ω c√≥ 3 ƒë·∫ßu ra
        id2label={0: "Bad", 1: "Neutral", 2: "Good"}, # G√°n nh√£n cho d·ªÖ hi·ªÉu khi infer
        label2id={"Bad": 0, "Neutral": 1, "Good": 2},
        ignore_mismatched_sizes=True
    )

    # # --- [CH√àN ƒêO·∫†N N√ÄY ƒê·ªÇ FIX L·ªñI BACKWARD & OOM] ---
    # print("Applying DeBERTa Gradient Checkpointing Fix...")
    
    # # 1. T·∫Øt cache (B·∫Øt bu·ªôc khi train)
    # model.config.use_cache = False 
    
    # # 2. B·∫≠t checkpointing th·ªß c√¥ng tr√™n model
    # model.gradient_checkpointing_enable()
    
    # # 3. THU·ªêC ƒê·∫∂C TR·ªä: ƒê·∫£m b·∫£o Input Embeddings nh·∫≠n Gradient
    # # N·∫øu kh√¥ng c√≥ d√≤ng n√†y, ƒë·ªì th·ªã t√≠nh to√°n s·∫Ω b·ªã ng·∫Øt qu√£ng g√¢y ra l·ªói "backward second time"
    # if hasattr(model, "enable_input_require_grads"):
    #     model.enable_input_require_grads()
    # else:
    #     def make_inputs_require_grad(module, input, output):
    #         output.requires_grad_(True)
    #     model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
    # # -----------------------------------------------------
    
    # Load dataset (B·∫°n c·∫ßn tr·ªè ƒë√∫ng file phase2_train.jsonl)
    full_dataset = PRMDataset("data/raw/phase1_train.jsonl", tokenizer, max_len=256)
    
    # Chia 90% train, 10% validation
    train_size = int(0.7 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}")

    # 5. Training Arguments
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,              # DeBERTa c·∫ßn train k·ªπ h∆°n ch√∫t (3-5 epochs)
        per_device_train_batch_size=8,   # 4 ho·∫∑c 8 t√πy VRAM (4 l√† an to√†n cho GPU 8-12GB)
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=4,   # T√≠ch l≈©y gradient ƒë·ªÉ batch size th·ª±c t·∫ø = 16
        gradient_checkpointing=False,     # <--- C·ª∞C K·ª≤ QUAN TR·ªåNG: Ti·∫øt ki·ªám 50-70% VRAM (ƒê·ªïi l·∫°i t·ªëc ƒë·ªô train s·∫Ω ch·∫≠m h∆°n kho·∫£ng 20%)
        # gradient_checkpointing_kwargs={"use_reentrant": False}, # <--- TH√äM D√íNG N√ÄY (Thu·ªëc ƒë·∫∑c tr·ªã)
        learning_rate=2e-5,              # QUAN TR·ªåNG: LR th·∫•p cho DeBERTa
        weight_decay=0.01,
        warmup_ratio=0.1,                # Warmup gi√∫p ·ªïn ƒë·ªãnh training ƒë·∫ßu
        lr_scheduler_type="cosine",      # <--- QUAN TR·ªåNG: Gi·∫£m LR theo h√¨nh Cosine (t·ªët h∆°n Linear)
        fp16=True,                       # TƒÉng t·ªëc tr√™n GPU
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=200,
        save_strategy="steps",
        save_steps=200,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        save_total_limit=2,              # Ch·ªâ gi·ªØ 2 checkpoint t·ªët nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám ·ªï c·ª©ng
        report_to="none",
        # Th√™m d√≤ng n√†y ƒë·ªÉ DataLoader load d·ªØ li·ªáu nhanh h∆°n
        dataloader_num_workers=4
    )

    # Collator padding ƒë·ªông
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = torch.argmax(torch.tensor(logits), dim=-1)
        acc = (predictions == torch.tensor(labels)).float().mean().item()
        return {"accuracy": acc}
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics
    )
    
    print("B·∫Øt ƒë·∫ßu hu√°n luy·ªán")
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("PRM Specialist Model trained and saved!")

if __name__ == "__main__":
    train() # Uncomment ƒë·ªÉ ch·∫°y
    pass